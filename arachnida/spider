#!/usr/bin/python3
# -*- coding: utf-8 -*-

import os
import logging
import sys
import requests
import traceback
from urllib.parse import urljoin, urlparse
from typing import MutableSet
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
session = requests.Session()
session.headers.update({"User-Agent": "Arachnida/1.0"})

cached_bodies = {}
VALID_EXTENSIONS = set(["jpg", "jpeg", "png", "gif", "bmp"])


def get_hostname(url: str) -> str:
    """
    Returns the hostname of a url, trimming also subdomains
    """
    try:
        return (
            urlparse(url).hostname.split(".")[-2]
            + "."
            + urlparse(url).hostname.split(".")[-1]
        )
    except:
        return None


def fix_link(href: str, protocol: str, hostname: str) -> str:
    if hostname is None:
        return href
    href = urljoin(protocol + "://" + hostname, href)
    return href.split("#")[0]


def get_links(
    url: str,
    recursive: bool = False,
    depth: int = 0,
    max_depth: int = 0,
    origin_hostname: str | None = None,
    filter: set = set(),
) -> MutableSet[str]:
    """
    Returns all lists in a webpage with these formats :
        - http...
        - https...
        - //...
        - /...
    """
    if depth > max_depth:
        return filter
    logging.info("Getting links in " + url)
    global cached_bodies
    if origin_hostname is None:
        origin_hostname = get_hostname(url)
    if origin_hostname is None:
        logging.warning("Skipping malformed url: " + url)
    data = session.get(url)
    # check if the page is a html page
    filter = set([url])
    if "text/html" not in data.headers["Content-Type"] or data.status_code != 200:
        return filter
    else:
        data = data.text
    cached_bodies[url] = data
    if (
        not recursive
    ):  # don't bother parsing the page if we're not searching recursively
        return filter
    soup = BeautifulSoup(data, "html.parser")
    # find all <a> or <link> tags
    for tag in soup.find_all(["a", "link"]):
        href = tag.get("href")
        if href is None:
            continue
        protocol = urlparse(url).scheme
        href = fix_link(href, protocol, get_hostname(url))
        if not href.startswith("http"):
            continue
        if get_hostname(href) != origin_hostname:
            logging.info("Skipping link to external website: " + href)
            continue
        if (
            href.startswith("http")
            and recursive
            and href not in filter
            and depth < max_depth
        ):
            filter.update(
                get_links(
                    href, recursive, depth + 1, max_depth, origin_hostname, filter
                )
            )
        filter.add(fix_link(href, protocol, get_hostname(url)))
    return filter


def get_images(urls: MutableSet[str], root_path: str):
    global cached_bodies
    for url in urls:
        try:
            data = cached_bodies.get(url, None)
            if data is None:  # non-200 status code
                continue
            logging.info("Getting images in " + url)
        except:
            logging.error("Failed to fetch body of {}".format(url))
            continue
        # find each src and srcset
        soup = BeautifulSoup(data, "html.parser")
        protocol = urlparse(url).scheme
        to_download = set()
        for tag in soup.find_all(["img", "source"]):
            src = tag.get("src")
            srcset = tag.get("srcset")
            if src is not None and src.split(".")[-1] in VALID_EXTENSIONS:
                to_download.add(fix_link(src, protocol, get_hostname(url)))
            if srcset is not None:
                for src in srcset.split(","):
                    if src.split(".")[-1] in VALID_EXTENSIONS:
                        to_download.add(
                            fix_link(
                                src.split(" ")[0].strip(), protocol, get_hostname(url)
                            )
                        )
        for image in to_download:
            if image is None:
                continue
            url = image
            path = urlparse(url).path
            output_dir = "{}/{}".format(root_path, path)
            os.makedirs(output_dir, exist_ok=True)
            try:
                with open("{}/{}".format(output_dir, path.split("/")[-1]), "wb+") as f:
                    logging.info("Downloading " + url + " to " + output_dir)
                    f.write(session.get(url).content)
            except KeyboardInterrupt:
                raise
            except:
                try:
                    os.remove("{}/{}".format(output_dir, path.split("/")[-1]))
                except KeyboardInterrupt:
                    raise
                except:
                    logging.error("Failed to download {}".format(url))
                    continue


class Spider:
    def _check_url(self) -> bool:
        try:
            session.head(self.url)
        except:
            return False
        return True

    def __init__(self):
        self.recursive = False
        self.depth = 5
        self.path = "./data"
        self.url = sys.argv[-1].strip()
        length = len(sys.argv)
        for i, arg in enumerate(sys.argv):
            if arg == "-r":
                self.recursive = True
            if arg == "-l":
                # check if next arg exists, and is a number
                if length > i + 1 and sys.argv[i + 1].isdigit():
                    self.depth = int(sys.argv[i + 1])
                else:
                    print("Error: -l must be followed by a positive number")
                    sys.exit(1)
            if arg == "-p":
                if length > i + 1 and i + 1 != length - 1:
                    self.path = sys.argv[i + 1]
                else:
                    print("Error: -p must be followed by a path")
                    sys.exit(1)
        if not self._check_url():
            print(
                "Error: URL is not valid, make sure it starts with http:// or https://"
            )
            sys.exit(1)
        logging.debug(f"recursive: {self.recursive}")
        logging.debug(f"depth: {self.depth}")
        logging.debug(f"path: {self.path}")
        logging.debug(f"url: {self.url}")

    def crawl(self):
        """
        Crawl recursively (or not) a webpage
        """
        try:
            links = get_links(self.url, self.recursive, 0, self.depth)
        except Exception:
            print("Failed to crawl the webpage")
            traceback.print_exc()
            return []
        logging.debug(f"links count: {len(links)}")
        logging.debug(f"links: {links}")
        try:
            get_images(links, self.path)
        except KeyboardInterrupt:
            pass
        except:
            pass


if __name__ == "__main__":
    if len(sys.argv) == 1:
        print(f"Usage: {sys.argv[0]} [-rlp] URL")
        sys.exit(0)
    spider = Spider()
    spider.crawl()
